We would like to thank the reviewers for their encouraging feedback and insightful comments. Almost all of the reviewer comments were positive. Below we reply to one comment and the one suggestion for a potential change to the paper.



REVIEWER 1

> On the other side, this paper seems a bit empirical, in
> the sense that experiments are carried out on some manual 
> decisions (such as split of source and target tasks).

Admittedly, it was our choice to perform separation into natural vs. man-made subsets. However, given the hierarchical class structure of WordNet and ImageNet, these two subsets were the only two large exclusive subsets available. This can be seen in the below list of the top 14 subsets of classes, sorted by number of classes (first column) in each subset:

158 n02075296 - carnivore
212 n01886756 - placental, placental mammal, eutherian, eutherian mammal
218 n01861778 - mammal, mammalian
337 n01466257 - chordate
337 n01471682 - vertebrate, craniate
358 n03575240 - instrumentality, instrumentation
398 n00015388 - animal, animate being, beast, brute, creature, fauna
410 n00004258 - living thing, animate thing
410 n00004475 - organism, being
522 n00021939 - artifact, artefact
949 n00003553 - whole, unit
958 n00002684 - object, physical object
997 n00001930 - physical entity
1000 n00001740 - entity

Starting at the largest subset "1000 n00001740 - entity" containing all 1000 classes and working backward, the first subset containing approximately 50% of the classes is "522 n00021939 - artifact, artefact", and the next is "410 n00004475 - organism, being". Fortunately for this study, these two subsets are mutually exclusive, the first became our "man-made" category and the second our "natural" category. Our only real influence was in choosing what to do with the remaining 1000-522-410=68 classes (Are cooked squash natural or man-made?). We looked at example images from each of the 68 classes and placed them into either "natural" or "man-made", as we thought best. In our code release, we provide a list of the 551 vs 449 classes in each category, and we highlight those 68 category decisions that were made manually.

We initially left most of this discussion out of the paper to save space, but thanks to this comment, we realize that in its absence the decisions do seem arbitrary. Thus, we will include this discussion in the final version.

> It would be good to see a quantitative criterion analyzing
> the correlation between the two factors raised in the
> paper and the final performance. Of course, the authors
> have fairly acknowledged so, and have not overclaimed
> anything beyond what is presented.

This is an interesting idea, though we're not sure exactly what sort of quantitative criterion would be most helpful. Could you elaborate on what you envision? We would be happy to attempt and include such an analysis once we understand exactly what is being suggested.



To summarize, we plan to make the following changes for a final version of the paper:

 - Add discussion of motivation behind creation of natural vs. man-made split, suggested by Reviewer 1.

 - Possibly add model suggested by Reviewer 1.

 - Strengthen statistical conclusions by averaging over more randomly initialized trials (trials currently running).
