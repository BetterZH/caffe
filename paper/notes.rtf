{\rtf1\ansi\ansicpg1252\cocoartf1187\cocoasubrtf400
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid101\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid201\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid301\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid401\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid501\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}}
\margl1440\margr1440\vieww20260\viewh16600\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b\fs30 \cf0 Outline\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b0\fs24 \cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural
\ls1\ilvl0\cf0 {\listtext	\uc0\u8259 	}Intro\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural
\ls1\ilvl1\cf0 {\listtext	\uc0\u8259 	}The state of the field (great progress recently)\
{\listtext	\uc0\u8259 	}It's kind of cool that some things work well but we dont' even quite understand why. (Side note: this is a bit of a departure from traditional ML, where we just push to improve performance. Now we have some stuff that works and want to study why)\
{\listtext	\uc0\u8259 	}To this end, we've done some experiments:\
{\listtext	\uc0\u8259 	}Experiment: How generic are features?\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural
\ls1\ilvl2\cf0 {\listtext	\uc0\u8259 	}This is a tricky question to pose. What do we even mean? Generic or specific with respect to what?\
{\listtext	\uc0\u8259 	}See also Donahue's work\
{\listtext	\uc0\u8259 	}We defined generic as transferring to a set of classes outside the dataset.\
{\listtext	\uc0\u8259 	}We tried two splits\
{\listtext	\uc0\u8259 	}Key findings: <insert info from lisa email here. Update for second experiment.>\
\pard\tx940\tx1440\tx1680\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural
\ls1\ilvl1\cf0 {\listtext	\uc0\u8259 	}Speed notes on training on top of generic features.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural
\ls1\ilvl2\cf0 {\listtext	\uc0\u8259 	}Compare to DeCaf paper? Speed is actually faster than expected??\
\pard\tx940\tx1440\tx1680\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural
\ls1\ilvl1\cf0 {\listtext	\uc0\u8259 	}Experiment: overfit with dataset size\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural
\ls1\ilvl2\cf0 {\listtext	\uc0\u8259 	}Couldn't really find much on this, surprisingly. Time to do the experiment!\
\pard\tx940\tx1440\tx1680\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural
\ls1\ilvl1\cf0 {\listtext	\uc0\u8259 	}Extra surprising result:\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural
\ls1\ilvl2\cf0 {\listtext	\uc0\u8259 	}coadaptation of middle units!\
\pard\tx220\tx720\tx1120\tx1680\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural
\ls1\ilvl0\cf0 {\listtext	\uc0\u8259 	}Method details\
\pard\tx940\tx1440\tx1680\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural
\ls1\ilvl1\cf0 {\listtext	\uc0\u8259 	}\uc0\u8730  We went with the architecture as in Alex's paper. Even though better ones exist (i.e. 7x7 from Matt Zeiler), we thought we'd go with the architecture used by the largest number of people, so our results would be comparable and extensible and useful to the most researchers. We actually expect none of the qualitative results presented here to depend at all on small architecture tweaks, though this is only a hunch\
{\listtext	\uc0\u8259 	}\uc0\u8730  Concretely, here are the layer sizes, parameters, etc that we used (table here)\
{\listtext	\uc0\u8259 	}X Also, why not: here's a table of layer activation and weight sizes (with biases separately). We don't all need to work this out on paper separately. Note: this may be slightly different than Alex's due to LRN diffs.\
{\listtext	\uc0\u8259 	}\uc0\u8730  All code and parameter files are available online at github (URL removed for review)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural
\ls1\ilvl0\cf0 {\listtext	\uc0\u8259 	}Experiments and results\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural
\ls1\ilvl1\cf0 {\listtext	\uc0\u8259 	}XXX Reduced Data\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural
\ls1\ilvl2\cf0 {\listtext	\uc0\u8259 	}Here's how much we overfit.\
{\listtext	\uc0\u8259 	}Figure + table. In table include total dataset size (and in figure??)\
{\listtext	\uc0\u8259 	}One conclusion: can clearly see that even more data would be great!\
{\listtext	\uc0\u8259 	}Even with one training example per class we get 4x better than chance.\
\pard\tx940\tx1440\tx1680\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural
\ls1\ilvl1\cf0 {\listtext	\uc0\u8259 	}Main Transfer results\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural
\ls1\ilvl2\cf0 {\listtext	\uc0\u8259 	}transfer falls off, not much\
{\listtext	\uc0\u8259 	}transfer fails for optimization reasons, not just for reasons of dataset mismatch\
{\listtext	\uc0\u8259 	}the dataset mismatch can actually help!\
\pard\tx940\tx1440\tx1680\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural
\ls1\ilvl1\cf0 {\listtext	\uc0\u8259 	}NatMan split\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural
\ls1\ilvl2\cf0 {\listtext	\uc0\u8259 	}Show filters from base / Nat / man??\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural
\ls1\ilvl0\cf0 {\listtext	\uc0\u8259 	}Discussion and Conclusions\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural
\ls1\ilvl1\cf0 {\listtext	\uc0\u8259 	}Cool stuff! This means that one can copy the first 4 layers.\
\pard\tx560\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
\pard\tx560\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b\fs30 \cf0 Cites\
\pard\tx560\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b0\fs24 \cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural
\ls2\ilvl0\cf0 {\listtext	\uc0\u8259 	}Zeiler, arXiv 2013, Visualizing and Understanding Convolutional Networks\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural
\ls2\ilvl1\cf0 {\listtext	\uc0\u8259 	}ablation study in Table 3. Some weirdness about overfit, etc...\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b\fs30 \cf0 Transfer Section
\b0\fs24 \
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural
\ls3\ilvl0\cf0 {\listtext	\uc0\u8259 	}Random dataset\
{\listtext	\uc0\u8259 	}Nat vs. man dataset\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural
\ls3\ilvl1\cf0 {\listtext	\uc0\u8259 	}449 vs 551\'85 see notebook\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b\fs30 \cf0 Overfitting Section\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b0\fs24 \cf0 \
Krizhevsky:\
The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4.\
\
\
\
\
\
\
\pard\tx560\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b\fs30 \cf0 TODO later, maybe\
\pard\tx560\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b0\fs24 \cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural
\ls4\ilvl0\cf0 {\listtext	\uc0\u8259 	}top5 error in addition to top1\
{\listtext	\uc0\u8259 	}Make sure reduced plot is made with 1300-2 run when finished\
{\listtext	\uc0\u8259 	}Provide list of which classes were in nat/man split (and/or show some examples from s/caffe/notebooks/natman_neither)\
{\listtext	\uc0\u8259 	}For arXiv version: include table of all transfer results (so we can see the yellow random filter line, particularly that it goes to ~0 in mid layers but ~4% at last layer)\
{\listtext	\uc0\u8259 	}Add acknowledgements (funding, UYWO cluster folks)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
Jason's todos started 8/4\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural
\ls5\ilvl0\cf0 {\listtext	\uc0\u8259 	}Abstract flows horribly. Fix.\
{\listtext	\uc0\u8259 	}Table 1: position somewhere besides in the middle of subpoint 5. \
{\listtext	\uc0\u8259 	}Table 1: make better caption\
{\listtext	\uc0\u8259 	}Fig 2: better caption. Refer to points in section 4.1, and give main results anyway (just say them without too much description). Mention x shift for visual clarity.\
{\listtext	\uc0\u8259 	}Add a few more refs. (http://arxiv.org/abs/1403.6382), Ask Y for more?\
{\listtext	\uc0\u8259 	}Figure 3: change "normalized" to "relative"\
{\listtext	\uc0\u8259 	}Maybe reference my 2012 RBM work??\
{\listtext	\uc0\u8259 	}The reviewers liked the paper after reading the whole thing. This is good news, but we should also optimize for people who only read the abstract (clear main points so they'll come back later), only read intro, etc\
{\listtext	\uc0\u8259 	}Fix colors in plots, make readable in black and white\
{\listtext	\uc0\u8259 	}Add supplemental material showing examples from nat/man split?\
{\listtext	\uc0\u8259 	}Fix: "showed, \'97 quite"\
{\listtext	\uc0\u8259 	}Fix: "dataset; and it is natural"\
{\listtext	\uc0\u8259 	}Note total number of FLOPs in study? Maybe point out how such studies are intensive and thus should be shared so everyone does not have to repeat them.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
Jason's todos from reading reviews starting 8/4\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural
\ls6\ilvl0\cf0 {\listtext	\uc0\u8259 	}Reviewer 3: "e.g., joint training of a network  leads to co-adaptation if too many layers are kept frozen, so that performance decreases even if the same part of the training set is used to re-train." This isn't quite the right way to put it. Maybe this part was confusing.\
{\listtext	\uc0\u8259 	}Reviewer 3: "And an exciting result is that pre-training on another training set, then training the whole network on the final training set, gives the best performance of all." Clearly give possible interpretations for this, i.e. seeing more data total (if not already in there\'85)\
{\listtext	\uc0\u8259 	}Reviewer 1:"experiments are carried out on some manual decisions (such as split of source and target tasks)." I believe the "manual decision" referred to is only the natural-vs-manmade split of the dataset, as opposed to the random 500 class splits, which aren't manual. If this is the case, then I could additionally justify this manual decision as the only reasonable one. Given the hierarchical structure of WordNet and ImageNet, these are the only large exclusive subsets available. In the top 25 subsets of classes, the only way to split the dataset approximately in half is as I've done (see [2] below). Mention this.\
{\listtext	\uc0\u8259 	}Reviewer 2: "Many interesting observations are made which were not previously reported, particularly results #3 and #5 in section 4.1."\
{\listtext	\uc0\u8259 	}Add discussion of nat/man splits, per rebuttal.\
\pard\tx560\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
\
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b\fs30 \cf0 Log\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b0\fs24 \cf0 [Mon   Aug 04, 2014   12:43 pm] Got NIPS reviews\
\
\
\
\
\
\
\
\
\
\
\
\
\
}