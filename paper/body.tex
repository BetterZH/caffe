%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

It has recently become possible to train much larger artificial neural networks than ever before. 

\editbox{write some intro fluff here vvvv}

Recent progress, enabled by Convnets \cite{LeCun89}

these new large networks are breaking previous performance records handily. Also unsupervised work: \cite{Le-2011-ICA} trained large unsupervised network using...

Most of these use combinations of learned convolutional features, pooling over fixed local regions, and normalization of responses based on the responses of other units in the same layer (a basic form of inhibition or local competition, which has a similar effect to ``explaining away'')

(speculation) It is interesting to think that, while we previously fought in a regime where mostly we tried to push performance, and most small performance gains well understood -- that is, the field of machine learning was largely a art of synthesizing functions that worked better and better -- we may soon be entering an era of the field where we have things that are so large and work so well that the performance will outpace our understanding. This will push the  field into a decidedly different regime, one of almost observational science where we merely poke things that work magically well to slowly remove their mystique.

Grand speculation aside, here we seek to understand a few properties of one of the larger class of convolutional neural networks trained using supervised backprop.

((examples))
((diff datasets))
((but same features!))

(((( two gabor features from very different networks ))))

\editbox{write some intro fluff here ----}

Many trained networks reported recently in the literature --- unsupervised and supervised alike --- exhibit a curious aspect in common: they all learn features that resemble gabor filters on the first layer (see Figure~\ref{fig:layer1}). This happens with such frequency that not only is it no longer surprising, but to obtain anything else on a natural-image dataset causes suspicion of poorly chosen hyperparameters or a bug somewhere in one's code. In fact, this paper's authors are now used to checking for gabor features in the first layer simply to assure ourselves that the training is at least partially working.

Because finding gabor features on the first layer seems to occur regardless of the exact cost function and natural image dataset used, we may call these these features \emph{general}. On the other hand, we know that the last layer of a trained network will depend greatly on the chosen \{network model, dataset\} combination. For example, for networks trained toward a supervised classification objective, each output unit will be specific to a particular class, at least to the extent that training is successful. In this sense, let's say that the last layer features are \emph{specific}. So far these terms are only intuitively defined, but we shall give one possible more rigorous definition soon.

If the layer 1 features are general, and the last layer features are specific, we know that somewhere in the network there must be a transition from general to specific. This observation raises a few questions:

\begin{itemize}
	\item Can we quantify the degree to which a particular layer is general or specific?
	\item For a particular deep network, does the transition occur suddenly from layer $n$ to $n+1$, or is it spread out over several layers?
	\item Where does this transition take place: near the bottom, middle, or top of the network?
\end{itemize}

Once one has general features from some layer of a base network trained on a dataset, a common trick is to repurpose the features, or \emph{transfer} them, to another task. The process that is usually followed is to create a new network containing as its first $n$ layers the first $n$ layers of the base network, with the rest of the layers in the new network randomly initialized and trained toward the second task. When following this path, one can choose to backpropagate the errors from the new task into the base features to \emph{fine-tune} them to the new task, or they can be left frozen. In cases where the second dataset is smaller than the first, fine-tuning the base features may result in overfitting, so the features are often left frozen. On the other hand, if the second dataset is large enough, the base features can be fine-tuned to the new task. Of course, if the second dataset is very large, there would be no need to transfer because the base filters could just be learned from scratch. These two domains --- fine-tuned features or frozen features --- are distinct and are compared later.

In this paper we make several contributions:

\begin{enumerate}
\item We propose one way of answering the first question above through the degree to which features from one dataset transfer to another (Section~\ref{sec:definition}). We then train a large convnet on the ImageNet dataset and use our definition of generality to examine the general to specific transition (Section~\ref{sec:experiments}).
\item We experimentally show that the difficulty in using transferred features can be decomposed into two separate issues: optimization difficulties due to breaking co-adaptation between neurons on neighboring layers and the specificity of the features themselves. In different regions of the network, different effects dominate.
\item Finally, we find an interesting effect where fine-tuning filters from a different dataset can lead to better solutions, even when the datasets are the same size and training time is identical.
\end{enumerate}




\edit{talk a little about related work?}

subsection{Related Work (no heading, just group with Intro?)}

Others have loosely addressed the question of generaltiy by showing how features from layers 5/6/7 can be transferred to new tasks \cite{donahue+jia-2013-arxiv} but here we're aiming for something slightly different: to observe the general to specific transition more directly, if possible.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Defining Generality vs. Specificity}
\label{sec:definition}

We have noted the curious tendency of gabor features to show up in the first layer of neural networks trained on natural images for a large range of network architectures and cost functions. We say these features are general if they are useful for many different tasks or specific if they are useful only for a few. We can measure the specificity of a set of features by training it on one task \dA and then using it for another task \dB.

Of course, this definition depends on the distance between \dA and \dB. In this paper we take tasks \dA and \dB to be classifying different non-overlapping subsets of a particular dataset.  \edit{while varying the semantic distance between \dA and \dB?} This is only one possible way of sampling tasks \dA and \dB.

The dataset we use is from the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) \citep{imagenet_cvpr09}. It contains 1,281,167 labeled training images and 50,000 test images. Each image in the training and validation sets is labeled with one of 1000 classes.
To create tasks \dA and \dB, we randomly split the 1000 classes into two groups, \dA and \dB, each containing 500 classes and approximately half of the data, or about 645,000 examples each. We train one convolutional net on the half-dataset \dA and one on the half-dataset \dB. These networks, which we call \net{baseA} and \net{baseB}, are shown in the top two positions of Figure~\ref{fig:transfer}.

All layer sizes in \net{baseA} and \net{baseB}, as well as the networks described later in this section are the same, namely the sizes described in Section~\ref{} except for the last softmax layer, which outputs 500 classes instead of 1000.


We then choose an $n$ from $\{1, 2, \ldots, 7\}$ and train several networks. To make the discussion more concrete, let's pick for now $n=3$ (Figure~\ref{fig:transfer} also shows the $n=3$ case). First we train 

\begin{itemize}
\item A control network \net{R3B} with the first $3$ layers fixed to random filters and all higher layers trained to classify images from dataset \dB.
\end{itemize}

Then we define and train the following two networks

\begin{itemize}
\item A \emph{selffer} network \net{B3B} where the first $3$ layers are copied from \net{baseB} and frozen. The five higher layers are initialized randomly and trained toward dataset \dB. Here we're only using data from \dB and layers from a network \net{baseB}, that was trained on \dB. This may seem a little strange, but we actually need it as an additional control for the below transfer network.

\item A \emph{transfer} network \net{A3B} where the first $3$ layers are copied from \net{baseA} and frozen. The five higher layers are initialized randomly and trained toward dataset \dB. Intuitively, here we're copying the first $3$ layers from the wrong dataset, \dA,  and then trying to learn higher layer features on top of them to classify \dB. If this works well, we'll say that those first three layers are general, at least with respect to \dB, and if not, we'll say they are specific to dataset \dA
\end{itemize}

Finally, we also train fine-tuned versions of the two above networks:

\begin{itemize}
\item A \emph{selffer} network \net{B3B^+} just like \net{B3B}, but where all layers learn
\item A \emph{transfer} network \net{A3B^+} just like \net{A3B}, but where all layers learn
\end{itemize}

Figure~\ref{fig:transfer} shows the networks created for $n = 3$, but we repeated this process for all $n$ in $\{1, 2, \ldots, 7\}$\footnote{Note that $n=8$ doesn't make sense in any case: \net{B8B} is just \net{baseB}, \net{R8B} is just a completely retrained version of \net{baseB}, and \net{A8B} would have no hope of working, because nothing would be trained and the 500 softmax neurons would not match between \dA and \dB.} and in both directions, i.e. transferring from \dA to \dB and \dB to \dA.


\begin{figure}[htpb]
\begin{center}
\includegraphics[width=1\linewidth]{drawings/transfer.pdf}
\end{center}
\caption{\edit{Caption here...}}
\label{fig:transfer}
\end{figure}





\section{Details of Experiment}

Since \cite{Krizhevsky-2012} won the ImageNet 2012 competition, there has naturally been much interest and work toward tweaking hyperparameters of large convolutional models. For example, \cite{Zeiler+et+al-arxiv2013b} found that it is better to decrease the first layer filters sizes from $11x11$ to $7x7$ and to use a smaller stride of $2$ instead of $4$. When beginning this study we faced the decision of exactly which convolutional architecture to use for our experiments. We decided to use the original architecture from \cite{Krizhevsky-2012} not to maximize absolute performance, but to study transfer results on what is probably the most popular single architecture, so that our results will be comparable, extensible, and useful to the largest number of other researchers. We actually expect none of the qualitative results presented here to depend at all on small architecture tweaks.

To this end, we used the hyperparameters provided in the reference implementation of  \cite{Krizhevsky-2012} with Caffe, a library for training convolutional models on a GPU \cite{donahue+jia-2013-arxiv}. We followed \cite{donahue+jia-2013-arxiv} in making a few minor departures from \cite{Krizhevsky-2012}: we skip the data augmentation trick of adding random multiples of principle components of pixel RGB values, which produced only a $1\%$ improvement in the original paper, and we warped images to $256x256$ instead of scaling to keep the aspect ratio and then cropping. In another difference, we placed the Local Response Normalization layers just \emph{after} the pooling layers, instead of before them. As in previous studies, we use Dropout \citep{Hinton-et-al-arxiv2012} on fully connected layers except for the softmax output layer.


For clarity of presentation, we list all hyperparameter selections in two tables: layer sizes and locations in Table~\ref{tab:network_architecture} and weight initialization and learning rate details in Table~\ref{tab:learning_hyperparams}. In the first table, a feedforward pass proceeds from top to bottom, left to right. Because code is often more clear than text, we've also made all code and parameter files necessary to reproduce these experiments available on \url{http://github.com/...removed_for_review...}.


\begin{table}[t]
\caption{Convolutional Network architecture sizes and hyperparameters}
\label{tab:network_architecture}
\begin{center}
\begin{tabular}{|r|l|l|l|l|l|l|l|l|l|}
\hline
       &      & filter & number  &        &        &     &         & Maxpool        & LRN (size,        \\
name   & pad  & size   & outputs & groups & stride & R/S & Dropout & (width,stride) & $\alpha$,$\beta$) \\
\hline
conv1  &      & 11x11  & 96      & 1      & 4      & R   &         & 3, 2           & 5,0.0001,0.75     \\
conv2  & 2    & 5x5    & 256     & 2      & 1      & R   &         & 3, 2           & 5,0.0001,0.75     \\
conv3  & 1    & 3x3    & 384     & 1      & 1      & R   &         &                &                   \\
conv4  & 1    & 3x3    & 384     & 2      & 1      & R   &         &                &                   \\
conv5  & 1    & 3x3    & 256     & 2      & 1      & R   &         & 3, 2           &                   \\
fc6    &      &        & 4096    & 1      &        & R   & *       &                &                   \\
fc7    &      &        & 4096    & 1      &        & R   & *       &                &                   \\
fc8    &      &        & 1000    & 1      &        & S   &         &                &                   \\
\hline
\end{tabular}
\end{center}
\end{table}




\begin{table}[t]
\caption{Parameter initializations and learning rates}
\label{tab:learning_hyperparams}
\begin{center}
\begin{tabular}{|r|l|l|l|l|l|l|}
\hline
       & \multicolumn{3}{c|}{\bf Weight}     &  \multicolumn{3}{c|}{\bf Bias}       \\
\cline{2-7}
       & gaus   & LR-mult & decay &  const  & LR  & decay \\
\hline
conv1  & .01    & 1       & y     &  0      & 2   & n     \\
conv2  & .01    & 1       & y     &  1      & 2   & n     \\
conv3  & .01    & 1       & y     &  0      & 2   & n     \\
conv4  & .01    & 1       & y     &  1      & 2   & n     \\
conv5  & .01    & 1       & y     &  1      & 2   & n     \\
fc6    & .005   & 1       & y     &  1      & 2   & n     \\
fc7    & .005   & 1       & y     &  1      & 2   & n     \\
fc8    & .01    & 1       & y     &  0      & 2   & n     \\ 
\hline
\end{tabular}
\end{center}
\end{table}

We used stochastic gradient descent (SGD) with momentum. Each iteration of SGD used a batch size of 256, momentum 0.9, and multiplicative weight decay (for those weights with weight decay enabled) of 0.0005 per iteration.

For the bulk of the experiments in this paper, the master learning rate was started at 0.01, annealed over the course of training by dropping it by a factor of 10 every 100,000 iterations, and learning was stopped after 450,000 iterations. All training was done on NVidia K20 GPUs. Each iteration took about 1.7 seconds, meaning that the whole training procedure for a single network takes 9.5 days.

Because occupying a whole GPU for this long was cumbersome, we also devised a set of hyperparameters to allow faster learning by boosting the learning rate by 25\% to 0.0125, annealing by a factor of 10 after only 64,000 iterations, and stopping after 200,000 iterations. These selections were made after looking at the learning curves for the base case and estimating at which points in each region of constant learning rate the learning had plateaued and thus annealing could take place.  A typical learning curve is shown in Figure~\ref{typical_learning}.


\editbox{
Figure {typical\_learning}
(( make this figure, if important, once reduced-1300-2 run is done ))
}

Our base model attains a final top-1 error on the validation set of 42.5\%, about the same as the 42.9\% reported by \cite{donahue+jia-2013-arxiv} and 1.8\% worse than \cite{Krizhevsky-2012}, the latter difference probably due to the few minor training differences. We checked these values only to demonstrate that the network was converging reasonably. As our goal is not to improve the state of the art, but to investigate properties of transfer, we were content with this level of performance.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Transfer Results and Discussion}
\label{sec:experiments}


\begin{figure}[htpb]
\begin{center}
\includegraphics[width=1\linewidth]{plots/result_transfer_crop.pdf}
\includegraphics[width=1\linewidth]{plots/result_transfer_lines_crop.pdf}
\end{center}
\caption{\edit{Caption here...}}
\label{fig:results}
\end{figure}

The results of all transfer learning experiments are shown in condensed form in Figure~\ref{fig:results}. Each dot in the figure represents the average accuracy over the validation set for an entire trained network. The black dots above $n=0$ represent the accuracy of \net{baseA} or \net{baseB}. There are eight points, because we made four separate random \net{A}/\net{B} splits. Each dark blue dot represents an \net{AnA} network\footnote{Actually these dots represent \net{AnA} nets or \net{BnB} nets, the statistics of which are equivalent, because in both cases the net is trained on the same random 500 classes. To simplify notation, we'll just calling these \net{AnA} networks. Similarly, we've aggregated the \net{BnA} and \net{AnB} networks and just call them \net{BnA}}, that is, a network that was trained on dataset \dA, chopped off at layer $n$, and then whose upper layers were reinitialized and retrained toward dataset \dA while the lower layers remained frozen. The light blue points represent \net{AnA^+} networks, the same as \net{AnA} but where the lower layers were trained as well. Each dark red dot represents a \net{BnA} network, one whose first $n$ layers are copied from \net{baseB} and frozen, and then whose upper layers were randomly initialized and trained toward dataset \dA. The light red points represent \net{BnA^+} networks where all layers were fine-tuned.

In the top subplot plot we've shifted each type of point slightly to the left or write simply for visual clarity; if not for this shift they would all be shown directly above $n=1$, $n=2$, etc. and would overlap. In the bottom subplot we've average the results at each layer to show more clearly the overall trend and to highlight the three important effects.

It turns out we can conclude quite a lot from these results! In each of the following interpretations, we compare the performance to the base case (black dots).

\begin{enumerate}

\item The black \net{baseA} points show that the mean performance of a network simply trained to classify a random subset of 500 classes attains a top-1 accuracy of 0.625, or 37.5\% top-1 error. This is different than the 42.5\% top-1 error attained on the 1000 class network for two a combination of two reasons: it is pushed higher because the network is trained only on half of the data, which could lead to more overfit, and it is pushed lower because there are only 500 classes, so there are only half as many ways to make mistakes. It is this latter effect that dominates, leading to lower error overall.

\item The dark blue \net{AnA} points show a curious behavior. We can see at layer one that the performance is the same (as the \net{baseA} points), which was expected. If we learn eight layers of features, save the first layer of learned gabor features, and reinitialize the whole network and retrain it toward the same task, we'd expect to do just as well. This is also true for layer 2. Layers 3, 4, 5, and 6, particularly 4 and 5, behave somewhat differently. When we keep the first, say, 5 layers and retrain layers 6-8, performance suffers. This is evidence that the original network contained \emph{co-adapted features} on successive layers, that is features that interact with each other in a complex way such that this co-adaptation \emph{could not be relearned} by the upper layers. In other words, gradient descent was able to find a good solution the first time, but this was only possible because the layers were jointly trained. As we continue to the right, we see that layer 6 is nearly back to the base level of performance, as is layer 7. As we get closer and closer to the final 500-way softmax output layer (8), there is less to relearn, and apparently relearning these one or two layers is simple enough for gradient descent to find a good solution. Alternately, we may say that there is less co-adaptation of features between layers 6 and 7 or between 7 and 8.

We are not sure whether or not this effect has been previously observed in the literature. Certainly one would suspect that optimization difficulties might occur, but we do not believe it has been previously shown that such difficulties are worse in the middle of a network than near the bottom or top.

\item The light blue \net{AnA^+} points show that, as expected, when we allow the copied lower layer features to learn as well, we're able to find a solution just as good as the original solution. The whole network is trained, just initialized from a partial previous solution instead of completely randomly, which neither helps nor hurts performance.

\item The dark red \net{BnA} points show the effect we set out to measure in the first place: the transferability of features from one network to another at each layer. Layers one and two transfer almost perfectly from \net{B} to  \net{A}, giving evidence that at least for our two tasks, not only are the gabor features general, but the second layer features are as well. Layer three shows a slight drop, and layers 4-7 show a more significant drop in performance. Thanks to the \net{ANA} points, we can tell that this drop is from a combination of two separate effects: the drop from lost co-adaptation \emph{and} the drop from features which are less and less general. On layers 3, 4, and 5, the first effect dominates, whereas on layers 6 and 7 the first effect diminishes and the specificity of representation dominates the drop in performance.

Although examples of successful feature transfer have been reported elsewhere in the literature, to our knowledge these results have been limited to noticing that transfer from a given layer is much better than chance, e.g. noticing that the \net{BnA} points are much better than chance level performance (.002 for 500 classes). We believe this is the first time that the extent to which transfer is successful has been carefully quantified layer by layer and that the two separate effects have been decoupled, showing that each effect dominates in part of the regime.

\item The light red \net{BnA^+} points show perhaps the most surprising effect: that transferred features followed by fine-tuning result in networks that generalize better than those trained directly on the target dataset. We suspected that this effect might be simply due to longer total training time (450k base + 450k fine-tuned for \net{BnA^+} vs. 450k for \net{baseA}), but this cannot be the case, because the \net{AnA^+} networks are also trained for the same longer length of time and do not exhibit this boost in performance.

Thus, the most likely explanation is that even after 450k iterations of fine tuning (beginning with completely random top layers), the effects of having seen the alternate dataset still linger, boosting generalization performance. We found the fact that this effect can linger through so much retraining surprising. Further, this lingering boon to generalization seems not to depend very much on how much of the first network we keep to initialize the second network: keeping anywhere from one to seven layers produces improved performance, with slightly better performance as we keep more layers. The average boost across layers 1 to 7 is 1.6\% over the base case, and the average if we keep at least five layers is 2.1\%.\footnote{It is somewhat strange for us to aggregate the performance over several layers. Indeed, we'd prefer to average many runs at the same layer, but each point is rather computationally expensive to obtain so we have very few as of the time of this submission. However, we think some aggregation is informative, because the performance at each layer is based on completely different random draws of initialization weights. Thus, the fact that layers 5, 6, and 7 result in almost identical performance across random draws is at least somewhat indicative that multiple runs at a given layer would result in similar performance.} The degree of performance boost is shown in Table~\ref{tab:boost}.

\end{enumerate}

% Computed like this (set LAYER = {1,3,5})
%
% LAYER = 1
% nested = [list(mb_selffer_ft[ii]['valid_top1_acc']) for ii in range(LAYER,8)]
% case_selffer = array([item for sublist in nested for item in sublist])
% 
% nested = [list(mb_transfer_ft[ii]['valid_top1_acc']) for ii in range(LAYER,8)]
% case_transfer = array([item for sublist in nested for item in sublist])
% 
% print 'transfer - selffer', case_transfer.mean() - case_selffer.mean()
% print 'transfer - base',    case_transfer.mean() - case_base.mean()
% 
% #print stats.ttest_rel(case_base, case_selffer)
% print 'base    vs selffer ', stats.ttest_ind(case_base, case_selffer)
% #print stats.ttest_rel(case_base, case_transfer)
% print 'base    vs transfer', stats.ttest_ind(case_base, case_transfer)
% print 'selffer vs transfer', stats.ttest_ind(case_selffer, case_transfer)


\begin{table}[t]
\caption{Performance accuracy boost of transfer + fine-tuning \net{BnA^+} over controls. All differences are significant at the $p < .001$ level.}
\label{tab:boost}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
           & mean boost  & mean boost \\
layers     & over        & over \\
aggregated & \net{baseA} & selffer \net{AnA^+} \\
\hline
1-7        & 1.6\%       & 1.4\% \\
3-7        & 1.8\%       & 1.4\% \\
5-7        & 2.1\%       & 1.7\% \\
\hline
\end{tabular}
\end{center}
\end{table}









\section{Conclusions}



Practical conclusions

If you don't have a big second dataset:

Results 2 and 4 imply that those wishing to transfer features from one dataset to another face not only the problem of specificity, but....



If you do have a big second dataset:




A final thing to notice is that all layers are fairly general. Even the networks in column 7, in which only the final 500-way softmax layer is retrained to the new dataset, suffer only an 8\% accuracy drop.

Caveat: We actually have to be careful here. These results only show that a random split of half of ImageNet transfers / generalizes to the other half. However, there are many related classes in the dataset. For example, here are 13 felids:

\begin{verbatim}
   n02123045 tabby, tabby cat
   n02123159 tiger cat
   n02123394 Persian cat
   n02123597 Siamese cat, Siamese
   n02124075 Egyptian cat
   n02125311 cougar, puma, catamount, mountain lion, Felis concolor
   n02127052 lynx, catamount
   n02128385 leopard, Panthera pardus
   n02128757 snow leopard, ounce, Panthera uncia
   n02128925 jaguar, panther, Panthera onca, Felis onca
   n02129165 lion, king of beasts, Panthera leo
   n02129604 tiger, Panthera tigris
   n02130308 cheetah, chetah, Acinonyx jubatus
\end{verbatim}

Because each A/B half will probably contain 6 or 7 of these, each half will have detectors trained on all levels to classify some types of felids. When generalizing to the other half, we would expect that the new high-level felid detectors trained on top of fixed old low-level felid detectors would work well. With this in mind, I've made a special manual 449-class vs 515-class split of the dataset into "natural" and "man made" categories to show how mid-level features do or do not transfer across this larger generalization gap (experiments still running).






Part 2 of results:

There's a little detail of the plot in the first email that bugged me. I expected monotonic performance drop as we move to the right on the plot, that is, as we copy more and more layers from the wrong dataset, accuracy should decrease.

However, if you look at layers 5 and 6, you'll notice that transfer performance when the first 5 layers are trained on A and the rest are trained on B is actually slightly *worse* than when we copy the first 6 layers. It's better to copy an extra layer of filters learned from the wrong dataset. It's just a tiny bump, but why?

It turns out, I think, that the bump is due to some interesting optimization difficulties. To expose them, I repeated the transfer experiments, but this time transferred the A filters to a new network and then retrained it on A. Concretely:
 - train baseA on A
 - copy the first N layers of baseA to a new network transferANA and freeze them
 - randomly initialize the higher layers of transferANA and train only those layers toward A

(Note: we could have just done this on the whole dataset, not the A half, but then any observed effects would have been confounded with effects due to differing dataset size)

This process gives the following results (blue dots show the performance of transferANA for various N)

+-------------+
|                    |
|                    |
+-------------+
(((( Figure 2 with blue and red dots ))))

Conclusions (from red and blue points):

 - Now we can reinterpret the red points as dropping from maximum performance for two separate reasons: optimization difficulties (drop in blue curve) and dataset mismatch (gap between blue and red). The gap between blue and red shows the dataset mismatch portion of the performance drop, and it's monotonically getting larger, as initially expected. It's just tricky to measure on its own.


Conclusions (just from blue points):

 - If we chop the network in half in the early layers (1,2,3) and retrain the upper part of it, gradient descent is able to find as good a solution as if the whole network is trained from scratch (expected).

 - Similarly for chopping the network at the upper layers (6,7) (expected).

 - However, if we chop the network after layer 4 or 5, gradient descent cannot recover as good of a solution as was found initially when everything was randomized. This seems to point toward a type of coadaptation of units across layers that can only be found when the units are trained together.

Is this later point surprising? It was to me. Has this effect been observed elsewhere?











\subsection{Control Experiment -- random filters}

((do this later... maybe put at end of Exchange section instead? ))

Compare to Jarret 2009 (first two layers don't work great...)
\cite{Jarrett-ICCV2009}








\section{Conclusions}

\edit{write this}
